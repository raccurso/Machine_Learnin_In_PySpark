{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Machine Learning con PySpark </center>\n",
    "\n",
    "#### Autor: Rodrigo Accurso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de esta práctica es la realización de un proceso completo de aprendizaje automático. Está desarrollada con PySpark, utilizando las librerias ML para DataFrames.\n",
    "\n",
    "El dataset se puede descargar directamente de Kaggle con este link:\n",
    "\n",
    "https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset\n",
    "\n",
    "La variable objetivo es categórica y posee dos valores (0,1), por lo cual el modelo a entrenar es de tipo clasificación binaria.\n",
    "\n",
    "He creado 5 pipelines para comparar los resultados obtenidos aplicando técnicas de selección y extracción de variables, o ninguna de las dos:\n",
    "1. Sin métodos de selección o extracción de variables (Base)\n",
    "2. Selección de variables con Chi-cuadrado y ranking con umbral p-valor <= 0.05\n",
    "3. Selección de variables con Chi-cuadrado y ranking con umbral p-valor <= 0.01\n",
    "4. Extracción de variables con PCA y k = 5\n",
    "5. Extracción de variables con PCA y k = 10\n",
    "\n",
    "Para cada uno de estos pipelines, he generado modelos de clasificación con los siguientes algoritmos:\n",
    "* Bayes Ingénuo\n",
    "* Support Vector Machienes\n",
    "* Random Forest\n",
    "* Gradient Boosting\n",
    "* Redes Neuronales\n",
    "\n",
    "El procedimiento de evaluación consiste en la validación cruzada con 5 carpetas. La métrica principal es el área bajo la curva ROC, pero también he calculado la exactitud del modelo final.\n",
    "\n",
    "El tuning de los hiper-parámetros se basa en el método del Grid Search, o sea asigno una serie de valores a cada uno y pruebo todas las combinaciones posibles para obtener el mejor resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido\n",
    "1. [Análisis exploratorio](#analisis-exploratorio)\n",
    "2. [Ingeniería de variables](#ingenieria-de-variables)\n",
    "3. [Bayes-Ingénuo](#bayes-ingenuo)\n",
    "4. [Support Vector Machines](#svm)\n",
    "5. [Random Forest](#random-forest)\n",
    "6. [Gradient Boosting Tree](#gbt)\n",
    "7. [Multilayer Perceptron](#perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Análisis exploratorio \n",
    "<a class=\"anchor\" id=\"analisis-exploratorio\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Import de las librerias y lectura del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import isnan, when, count, countDistinct\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import NaiveBayes, LinearSVC, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.feature import ChiSqSelector, PCA\n",
    "import numpy as np\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"PracticaFinal\")\n",
    "sql = pyspark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1470\n",
      "35\n",
      "[Row(Age=41, Attrition='Yes', BusinessTravel='Travel_Rarely', DailyRate=1102, Department='Sales', DistanceFromHome=1, Education=2, EducationField='Life Sciences', EmployeeCount=1, EmployeeNumber=1, EnvironmentSatisfaction=2, Gender='Female', HourlyRate=94, JobInvolvement=3, JobLevel=2, JobRole='Sales Executive', JobSatisfaction=4, MaritalStatus='Single', MonthlyIncome=5993, MonthlyRate=19479, NumCompaniesWorked=8, Over18='Y', OverTime='Yes', PercentSalaryHike=11, PerformanceRating=3, RelationshipSatisfaction=1, StandardHours=80, StockOptionLevel=0, TotalWorkingYears=8, TrainingTimesLastYear=0, WorkLifeBalance=1, YearsAtCompany=6, YearsInCurrentRole=4, YearsSinceLastPromotion=0, YearsWithCurrManager=5), Row(Age=49, Attrition='No', BusinessTravel='Travel_Frequently', DailyRate=279, Department='Research & Development', DistanceFromHome=8, Education=1, EducationField='Life Sciences', EmployeeCount=1, EmployeeNumber=2, EnvironmentSatisfaction=3, Gender='Male', HourlyRate=61, JobInvolvement=2, JobLevel=2, JobRole='Research Scientist', JobSatisfaction=2, MaritalStatus='Married', MonthlyIncome=5130, MonthlyRate=24907, NumCompaniesWorked=1, Over18='Y', OverTime='No', PercentSalaryHike=23, PerformanceRating=4, RelationshipSatisfaction=4, StandardHours=80, StockOptionLevel=1, TotalWorkingYears=10, TrainingTimesLastYear=3, WorkLifeBalance=3, YearsAtCompany=10, YearsInCurrentRole=7, YearsSinceLastPromotion=1, YearsWithCurrManager=7)]\n"
     ]
    }
   ],
   "source": [
    "df = sql.read.csv('HR.Employee.Attrition.csv', sep=\",\", inferSchema=True, header=True)\n",
    "print(df.count())\n",
    "print(len(df.columns))\n",
    "print(df.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Obtengo los tipos de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Attrition: string (nullable = true)\n",
      " |-- BusinessTravel: string (nullable = true)\n",
      " |-- DailyRate: integer (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- DistanceFromHome: integer (nullable = true)\n",
      " |-- Education: integer (nullable = true)\n",
      " |-- EducationField: string (nullable = true)\n",
      " |-- EmployeeCount: integer (nullable = true)\n",
      " |-- EmployeeNumber: integer (nullable = true)\n",
      " |-- EnvironmentSatisfaction: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- HourlyRate: integer (nullable = true)\n",
      " |-- JobInvolvement: integer (nullable = true)\n",
      " |-- JobLevel: integer (nullable = true)\n",
      " |-- JobRole: string (nullable = true)\n",
      " |-- JobSatisfaction: integer (nullable = true)\n",
      " |-- MaritalStatus: string (nullable = true)\n",
      " |-- MonthlyIncome: integer (nullable = true)\n",
      " |-- MonthlyRate: integer (nullable = true)\n",
      " |-- NumCompaniesWorked: integer (nullable = true)\n",
      " |-- Over18: string (nullable = true)\n",
      " |-- OverTime: string (nullable = true)\n",
      " |-- PercentSalaryHike: integer (nullable = true)\n",
      " |-- PerformanceRating: integer (nullable = true)\n",
      " |-- RelationshipSatisfaction: integer (nullable = true)\n",
      " |-- StandardHours: integer (nullable = true)\n",
      " |-- StockOptionLevel: integer (nullable = true)\n",
      " |-- TotalWorkingYears: integer (nullable = true)\n",
      " |-- TrainingTimesLastYear: integer (nullable = true)\n",
      " |-- WorkLifeBalance: integer (nullable = true)\n",
      " |-- YearsAtCompany: integer (nullable = true)\n",
      " |-- YearsInCurrentRole: integer (nullable = true)\n",
      " |-- YearsSinceLastPromotion: integer (nullable = true)\n",
      " |-- YearsWithCurrManager: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Verifico la presencia de NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Age=0, Attrition=0, BusinessTravel=0, DailyRate=0, Department=0, DistanceFromHome=0, Education=0, EducationField=0, EmployeeCount=0, EmployeeNumber=0, EnvironmentSatisfaction=0, Gender=0, HourlyRate=0, JobInvolvement=0, JobLevel=0, JobRole=0, JobSatisfaction=0, MaritalStatus=0, MonthlyIncome=0, MonthlyRate=0, NumCompaniesWorked=0, Over18=0, OverTime=0, PercentSalaryHike=0, PerformanceRating=0, RelationshipSatisfaction=0, StandardHours=0, StockOptionLevel=0, TotalWorkingYears=0, TrainingTimesLastYear=0, WorkLifeBalance=0, YearsAtCompany=0, YearsInCurrentRole=0, YearsSinceLastPromotion=0, YearsWithCurrManager=0)]\n"
     ]
    }
   ],
   "source": [
    "df_na = df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).collect()\n",
    "print(df_na)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ingeniería de variables\n",
    "<a class=\"anchor\" id=\"ingenieria-de-variables\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Elimino las columnas con valores diferentes en todas las filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Age='F', Attrition='F', BusinessTravel='F', DailyRate='F', Department='F', DistanceFromHome='F', Education='F', EducationField='F', EmployeeCount='F', EmployeeNumber='T', EnvironmentSatisfaction='F', Gender='F', HourlyRate='F', JobInvolvement='F', JobLevel='F', JobRole='F', JobSatisfaction='F', MaritalStatus='F', MonthlyIncome='F', MonthlyRate='F', NumCompaniesWorked='F', Over18='F', OverTime='F', PercentSalaryHike='F', PerformanceRating='F', RelationshipSatisfaction='F', StandardHours='F', StockOptionLevel='F', TotalWorkingYears='F', TrainingTimesLastYear='F', WorkLifeBalance='F', YearsAtCompany='F', YearsInCurrentRole='F', YearsSinceLastPromotion='F', YearsWithCurrManager='F')]\n"
     ]
    }
   ],
   "source": [
    "df_unique = df.select([when(countDistinct(column) == df.count(), 'T').otherwise('F').alias(column) for column in df.columns]) \\\n",
    "                .collect()\n",
    "print(df_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('EmployeeNumber')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Elimino las columnas con valor igual en todas las filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Age='F', Attrition='F', BusinessTravel='F', DailyRate='F', Department='F', DistanceFromHome='F', Education='F', EducationField='F', EmployeeCount='T', EnvironmentSatisfaction='F', Gender='F', HourlyRate='F', JobInvolvement='F', JobLevel='F', JobRole='F', JobSatisfaction='F', MaritalStatus='F', MonthlyIncome='F', MonthlyRate='F', NumCompaniesWorked='F', Over18='T', OverTime='F', PercentSalaryHike='F', PerformanceRating='F', RelationshipSatisfaction='F', StandardHours='T', StockOptionLevel='F', TotalWorkingYears='F', TrainingTimesLastYear='F', WorkLifeBalance='F', YearsAtCompany='F', YearsInCurrentRole='F', YearsSinceLastPromotion='F', YearsWithCurrManager='F')]\n"
     ]
    }
   ],
   "source": [
    "df_same = df.select([when(countDistinct(column) == 1, 'T').otherwise('F').alias(column) for column in df.columns]) \\\n",
    "                .collect()\n",
    "print(df_same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('EmployeeCount')\n",
    "df = df.drop('Over18')\n",
    "df = df.drop('StandardHours')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Convierto las variables alfanumericas en numericas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BusinessTravel',\n",
       " 'Department',\n",
       " 'EducationField',\n",
       " 'Gender',\n",
       " 'JobRole',\n",
       " 'MaritalStatus',\n",
       " 'OverTime']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_stages = []\n",
    "string_cols = [x[0] for x in df.dtypes if (x[1] == 'string') & (x[0] != 'Attrition')]\n",
    "string_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Age',\n",
       " 'DailyRate',\n",
       " 'DistanceFromHome',\n",
       " 'Education',\n",
       " 'EnvironmentSatisfaction',\n",
       " 'HourlyRate',\n",
       " 'JobInvolvement',\n",
       " 'JobLevel',\n",
       " 'JobSatisfaction',\n",
       " 'MonthlyIncome',\n",
       " 'MonthlyRate',\n",
       " 'NumCompaniesWorked',\n",
       " 'PercentSalaryHike',\n",
       " 'PerformanceRating',\n",
       " 'RelationshipSatisfaction',\n",
       " 'StockOptionLevel',\n",
       " 'TotalWorkingYears',\n",
       " 'TrainingTimesLastYear',\n",
       " 'WorkLifeBalance',\n",
       " 'YearsAtCompany',\n",
       " 'YearsInCurrentRole',\n",
       " 'YearsSinceLastPromotion',\n",
       " 'YearsWithCurrManager']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_cols = [x[0] for x in df.dtypes if x[1] != 'string']\n",
    "numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in string_cols:\n",
    "    indexer = StringIndexer(inputCol = col, outputCol = col + 'Index')\n",
    "    main_stages += [indexer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformo la variable target Attrition separadamente porque no debe estar en el pipeline\n",
    "indexer = StringIndexer(inputCol = 'Attrition', outputCol = 'label')\n",
    "indexer = indexer.fit(df)\n",
    "df = indexer.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Aplico el One Hot Encoding en las variables categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['Department', 'EducationField','JobRole','MaritalStatus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    encoder = OneHotEncoderEstimator(inputCols = [col + 'Index'], outputCols = [col + 'Vec'])\n",
    "    main_stages += [encoder]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Genero el vector necesario para entrenar los modelos de ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables numericas\n",
    "assemblerInputs = numeric_cols\n",
    "# Variables alfanumericas a las que no aplique el one hot encoding\n",
    "assemblerInputs = assemblerInputs + [col + 'Index' for col in (set(string_cols) - set(cat_cols))]\n",
    "# Variables alfanumericas a las que aplique el one hot encoding\n",
    "assemblerInputs = assemblerInputs + [col + 'Vec' for col in cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol='features')\n",
    "main_stages += [assembler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Normalizacion de las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(inputCol='features', outputCol='scaledFeatures')\n",
    "main_stages += [scaler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7 Selección de variables por Chi-cuadrado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selección con p-valor <= 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chisq_selector_05 = ChiSqSelector(fpr=0.05, selectorType='fpr',featuresCol='scaledFeatures',\n",
    "                         outputCol='selectedFeatures', labelCol='label')\n",
    "chisq_stages_05 = main_stages[:]\n",
    "chisq_stages_05 += [chisq_selector_05]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selección con p-valor <= 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chisq_selector_01 = ChiSqSelector(fpr=0.01, selectorType='fpr',featuresCol='scaledFeatures',\n",
    "                         outputCol='selectedFeatures', labelCol='label')\n",
    "chisq_stages_01 = main_stages[:]\n",
    "chisq_stages_01 += [chisq_selector_01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8 Extracción de variables con PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracción con k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_5 = PCA(k=5, inputCol='scaledFeatures', outputCol='pcaFeatures')\n",
    "pca_stages_5 = main_stages[:]\n",
    "pca_stages_5 += [pca_5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracción con k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_10 = PCA(k=10, inputCol='scaledFeatures', outputCol='pcaFeatures')\n",
    "pca_stages_10 = main_stages[:]\n",
    "pca_stages_10 += [pca_10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bayes Ingénuo\n",
    "<a class=\"anchor\" id=\"bayes-ingenuo\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Creo diccionarios con la información necesaria para ejecutar todos los casos por cada aloritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stages = {'BASE': main_stages,\n",
    "              'CHI-CUADRADO-05': chisq_stages_05,\n",
    "              'CHI-CUADRADO-01': chisq_stages_01}\n",
    "\n",
    "feature_field = {'BASE': 'scaledFeatures',\n",
    "                 'CHI-CUADRADO-05': 'selectedFeatures',\n",
    "                 'CHI-CUADRADO-01': 'selectedFeatures'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: No puedo utilizar PCA porque el Bayes Ingénuo no acepta números negativos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Creo una función que evalúa el algoritmo con los hiper-parámetros en entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evualua_modelo(input_smoothing):\n",
    "    evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "    for sel_stages in all_stages:\n",
    "        \n",
    "        print('CASO ' + sel_stages)\n",
    "        print('------------------------')\n",
    "        \n",
    "        # Creo el algoritmo de clasificación\n",
    "        nb = NaiveBayes(featuresCol=feature_field.get(sel_stages), labelCol='label')    \n",
    "        print('Features: ' + feature_field.get(sel_stages))\n",
    "        \n",
    "        # Construyo el pipeline completo\n",
    "        nb_stages = all_stages.get(sel_stages)[:]\n",
    "        nb_stages += [nb]\n",
    "        pipeline = Pipeline(stages=nb_stages)\n",
    "\n",
    "        # Creo el grid de hiper-parámetros\n",
    "        paramGrid = (ParamGridBuilder()\n",
    "                     .addGrid(nb.smoothing, input_smoothing.get(sel_stages))\n",
    "                     .addGrid(nb.modelType, ['multinomial'])\n",
    "                     .build())  \n",
    "\n",
    "        # Ejecuto la validación cruzada con los hiperparámetros seleccionados\n",
    "        cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, \n",
    "                            evaluator=evaluator, numFolds=5)\n",
    "        pipelineModel = cv.fit(df)    \n",
    "        \n",
    "        # Muestro los resultados\n",
    "        print('Hiper-parámetros óptimos:')\n",
    "        print('smoothing = ' + str(pipelineModel.bestModel.stages[-1]._java_obj.getSmoothing()))\n",
    "        print('ROC-AUC = ' + str(np.mean(pipelineModel.avgMetrics)))\n",
    "        print('')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Coarse-tuning de hiper-parámetros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASO BASE\n",
      "------------------------\n",
      "Features: scaledFeatures\n",
      "Hiper-parámetros óptimos:\n",
      "smoothing = 0.0\n",
      "ROC-AUC = 0.6305925693344068\n",
      "\n",
      "CASO CHI-CUADRADO-05\n",
      "------------------------\n",
      "Features: selectedFeatures\n",
      "Hiper-parámetros óptimos:\n",
      "smoothing = 0.0\n",
      "ROC-AUC = 0.6314127561352433\n",
      "\n",
      "CASO CHI-CUADRADO-01\n",
      "------------------------\n",
      "Features: selectedFeatures\n",
      "Hiper-parámetros óptimos:\n",
      "smoothing = 0.0\n",
      "ROC-AUC = 0.6334527955217494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_smoothing = {'BASE': [0., 0.5, 1.0],\n",
    "                   'CHI-CUADRADO-05': [0., 0.5, 1.0],\n",
    "                   'CHI-CUADRADO-01':  [0., 0.5, 1.0]}                   \n",
    "    \n",
    "evualua_modelo(param_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Fine-tuning de hiper-parámetros y evaluación de la ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASO BASE\n",
      "------------------------\n",
      "Features: scaledFeatures\n",
      "Hiper-parámetros óptimos:\n",
      "smoothing = 0.0\n",
      "ROC-AUC = 0.6310011695464576\n",
      "\n",
      "CASO CHI-CUADRADO-05\n",
      "------------------------\n",
      "Features: selectedFeatures\n",
      "Hiper-parámetros óptimos:\n",
      "smoothing = 0.0\n",
      "ROC-AUC = 0.6320403657626575\n",
      "\n",
      "CASO CHI-CUADRADO-01\n",
      "------------------------\n",
      "Features: selectedFeatures\n",
      "Hiper-parámetros óptimos:\n",
      "smoothing = 0.0\n",
      "ROC-AUC = 0.6338857821851124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_smoothing = {'BASE': [0., 0.05, .1],\n",
    "                   'CHI-CUADRADO-05': [0., 0.05, .1],\n",
    "                   'CHI-CUADRADO-01':  [0., 0.05, .1]}                   \n",
    "    \n",
    "evualua_modelo(param_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Calculo la exactitud del mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8435268000387981\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "nb = NaiveBayes(featuresCol='selectedFeatures', labelCol='label')\n",
    "\n",
    "nb_stages = chisq_stages_01[:]\n",
    "nb_stages += [nb]\n",
    "pipeline = Pipeline(stages=nb_stages)\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(nb.smoothing, [0.])\n",
    "             .build())  \n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, \n",
    "                    evaluator=evaluator, numFolds=5)\n",
    "model = cv.fit(df)  \n",
    "print('Accuracy = ' + str(np.mean(model.avgMetrics)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El único hiper-parámetro configurable es el Smoothing, que es la suavización de probabilidades a través del Estimador de Laplace. El valor óptimo es 0 porque en el dataset no existe el caso en que el valor de una variable categórica no se esté en una de las dos clases.\n",
    "\n",
    "La ejecución con selección de variables con p-valor <= 0.01 obtuvo el mejor resultado. Supongo que el motivo es la no independencia de variables que existe en el dataset, mientras que Bayes Ingénuo asume una completa independencia.\n",
    "\n",
    "ROC-AUC = 0.6339\n",
    "\n",
    "Accuracy = 0.8435"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Support Vector Machines\n",
    "<a class=\"anchor\" id=\"svm\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Creo diccionarios con la información necesaria para ejecutar todos los casos por cada aloritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stages = {'BASE': main_stages,\n",
    "              'CHI-CUADRADO-05': chisq_stages_05,\n",
    "              'CHI-CUADRADO-01': chisq_stages_01,\n",
    "              'PCA-k5': pca_stages_5,\n",
    "              'PCA-k10': pca_stages_10}\n",
    "\n",
    "feature_field = {'BASE': 'scaledFeatures',\n",
    "                 'CHI-CUADRADO-05': 'selectedFeatures',\n",
    "                 'CHI-CUADRADO-01': 'selectedFeatures',\n",
    "                 'PCA-k5': 'pcaFeatures',\n",
    "                 'PCA-k10': 'pcaFeatures'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Creo una función que evalúa el algoritmo con los hiper-parámetros en entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evualua_modelo_SVM(input_regParam, input_maxIter):\n",
    "    evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "    for sel_stages in all_stages:\n",
    "        \n",
    "        print('CASO ' + sel_stages)\n",
    "        print('------------------------')\n",
    "        \n",
    "        # Creo el algoritmo de clasificación\n",
    "        svc = LinearSVC(featuresCol=feature_field.get(sel_stages), labelCol='label')    \n",
    "        print('Features: ' + feature_field.get(sel_stages))\n",
    "        \n",
    "        # Construyo el pipeline completo\n",
    "        svc_stages = all_stages.get(sel_stages)[:]\n",
    "        svc_stages += [svc]\n",
    "        pipeline = Pipeline(stages=svc_stages)\n",
    "\n",
    "        # Creo el grid de hiper-parámetros\n",
    "        paramGrid = (ParamGridBuilder()\n",
    "                     .addGrid(svc.regParam, input_regParam.get(sel_stages))\n",
    "                     .addGrid(svc.maxIter, input_maxIter.get(sel_stages))                  \n",
    "                     .build())  \n",
    "\n",
    "        # Ejecuto la validación cruzada con los hiperparámetros seleccionados\n",
    "        cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, \n",
    "                            evaluator=evaluator, numFolds=5)\n",
    "        pipelineModel = cv.fit(df)    \n",
    "        \n",
    "        # Muestro los resultados\n",
    "        print('Hiper-parámetros óptimos:')\n",
    "        print('regParam = ' + str(pipelineModel.bestModel.stages[-1]._java_obj.getRegParam()))\n",
    "        print('maxIter = ' + str(pipelineModel.bestModel.stages[-1]._java_obj.getMaxIter()))\n",
    "        print('ROC-AUC = ' + str(np.mean(pipelineModel.avgMetrics)))\n",
    "        print('')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Coarse-tuning de hiper-parámetros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASO BASE\n",
      "------------------------\n",
      "Features: scaledFeatures\n",
      "Hiper-parámetros óptimos:\n",
      "regParam = 0.5\n",
      "maxIter = 100\n",
      "ROC-AUC = 0.8143945767212432\n",
      "\n",
      "CASO CHI-CUADRADO-05\n",
      "------------------------\n",
      "Features: selectedFeatures\n",
      "Hiper-parámetros óptimos:\n",
      "regParam = 1.0\n",
      "maxIter = 50\n",
      "ROC-AUC = 0.8002146337229715\n",
      "\n",
      "CASO CHI-CUADRADO-01\n",
      "------------------------\n",
      "Features: selectedFeatures\n",
      "Hiper-parámetros óptimos:\n",
      "regParam = 0.5\n",
      "maxIter = 100\n",
      "ROC-AUC = 0.798068156553817\n",
      "\n",
      "CASO PCA-k5\n",
      "------------------------\n",
      "Features: pcaFeatures\n",
      "Hiper-parámetros óptimos:\n",
      "regParam = 0.0\n",
      "maxIter = 10\n",
      "ROC-AUC = 0.654768298041164\n",
      "\n",
      "CASO PCA-k10\n",
      "------------------------\n",
      "Features: pcaFeatures\n",
      "Hiper-parámetros óptimos:\n",
      "regParam = 0.0\n",
      "maxIter = 10\n",
      "ROC-AUC = 0.7626341477018037\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_regParam = {'BASE': [0., 0.5, 1.0],\n",
    "                  'CHI-CUADRADO-05': [0., 0.5, 1.0],\n",
    "                  'CHI-CUADRADO-01':  [0., 0.5, 1.0],\n",
    "                  'PCA-k5': [0., 0.5, 1.0],\n",
    "                  'PCA-k10': [0., 0.5, 1.0]}\n",
    "\n",
    "param_maxIter = {'BASE': [10, 50, 100],\n",
    "                 'CHI-CUADRADO-05': [10, 50, 100],\n",
    "                 'CHI-CUADRADO-01': [10, 50, 100],\n",
    "                 'PCA-k5': [10, 50, 100],\n",
    "                 'PCA-k10': [10, 50, 100]}\n",
    "    \n",
    "evualua_modelo_SVM(param_regParam, param_maxIter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Fine-tuning de hiper-parámetros y evaluación de la ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASO BASE\n",
      "------------------------\n",
      "Features: scaledFeatures\n",
      "Hiper-parámetros óptimos:\n",
      "regParam = 0.6\n",
      "maxIter = 100\n",
      "ROC-AUC = 0.8293923941436913\n",
      "\n",
      "CASO CHI-CUADRADO-05\n",
      "------------------------\n",
      "Features: selectedFeatures\n",
      "Hiper-parámetros óptimos:\n",
      "regParam = 1.0\n",
      "maxIter = 50\n",
      "ROC-AUC = 0.8022375679279371\n",
      "\n",
      "CASO CHI-CUADRADO-01\n",
      "------------------------\n",
      "Features: selectedFeatures\n",
      "Hiper-parámetros óptimos:\n",
      "regParam = 0.5\n",
      "maxIter = 80\n",
      "ROC-AUC = 0.8046145747715915\n",
      "\n",
      "CASO PCA-k5\n",
      "------------------------\n",
      "Features: pcaFeatures\n",
      "Hiper-parámetros óptimos:\n",
      "regParam = 0.0\n",
      "maxIter = 10\n",
      "ROC-AUC = 0.6437187105127199\n",
      "\n",
      "CASO PCA-k10\n",
      "------------------------\n",
      "Features: pcaFeatures\n",
      "Hiper-parámetros óptimos:\n",
      "regParam = 0.1\n",
      "maxIter = 10\n",
      "ROC-AUC = 0.7625266621007054\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_regParam = {'BASE': [0.4, 0.5, 0.6],\n",
    "                  'CHI-CUADRADO-05': [0.8, 0.9, 1.0],\n",
    "                  'CHI-CUADRADO-01':  [0.4, 0.5, 0.6],\n",
    "                  'PCA-k5': [0., 0.05, 0.1],\n",
    "                  'PCA-k10': [0., 0.05, 0.1]}\n",
    "\n",
    "param_maxIter = {'BASE': [80, 100, 120],\n",
    "                 'CHI-CUADRADO-05': [30, 50, 70],\n",
    "                 'CHI-CUADRADO-01': [80, 100, 120],\n",
    "                 'PCA-k5': [10, 20, 30],\n",
    "                 'PCA-k10': [10, 20, 30]}\n",
    "    \n",
    "evualua_modelo_SVM(param_regParam, param_maxIter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Calculo la exactitud del mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8385337330124533\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "svc = LinearSVC(featuresCol='scaledFeatures', labelCol='label')\n",
    "\n",
    "svc_stages = main_stages[:]\n",
    "svc_stages += [svc]\n",
    "pipeline = Pipeline(stages=svc_stages)\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(svc.regParam, [0.6])\n",
    "             .addGrid(svc.maxIter, [100])  \n",
    "             .build())  \n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, \n",
    "                    evaluator=evaluator, numFolds=5)\n",
    "model = cv.fit(df)  \n",
    "print('Accuracy = ' + str(np.mean(model.avgMetrics)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6 Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parámetro de regularización en SVM sirve a penalizar las clasificaciones erradas. Más alto su valor, mayor será el precio a pagar en la función de evaluación. El valor óptimo encontrado es 0.01.\n",
    "El parámetro maxIter, en cambio, limita el número de iteraciones para el entrenamiento. El valor óptimo encontrado es 120.\n",
    "\n",
    "Entre los 5 escenarios probados, obtuve mejor resultado sin aplicar selección o extracción de variables.\n",
    "\n",
    "ROC-AUC = 0.8294\n",
    "\n",
    "Accuracy = 0.8385"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Random Forest\n",
    "<a class=\"anchor\" id=\"random-forest\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Creo diccionarios con la información necesaria para ejecutar todos los casos por cada aloritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stages = {'BASE': main_stages,\n",
    "              'PCA-k5': pca_stages_5,\n",
    "              'PCA-k10': pca_stages_10}\n",
    "\n",
    "feature_field = {'BASE': 'scaledFeatures',\n",
    "                 'PCA-k5': 'pcaFeatures',\n",
    "                 'PCA-k10': 'pcaFeatures'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: Random Forest es un algoritmo basado en árboles de decisión, los cuales realizan internamente la selección de variables. Por este motivo, excluyo los dos pipelines con selección de variables por chi-cuadrado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Creo una función que evalúa el algoritmo con los hiper-parámetros en entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evualua_modelo_RF(input_numTrees, input_maxDepth):\n",
    "    evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "    for sel_stages in all_stages:\n",
    "        \n",
    "        print('CASO ' + sel_stages)\n",
    "        print('------------------------')\n",
    "        \n",
    "        # Creo el algoritmo de clasificación\n",
    "        rf = RandomForestClassifier(featuresCol=feature_field.get(sel_stages), labelCol='label')\n",
    "        print('Features: ' + feature_field.get(sel_stages))\n",
    "        \n",
    "        # Construyo el pipeline completo\n",
    "        rf_stages = all_stages.get(sel_stages)[:]\n",
    "        rf_stages += [rf]\n",
    "        pipeline = Pipeline(stages=rf_stages)\n",
    "\n",
    "        # Creo el grid de hiper-parámetros\n",
    "        paramGrid = (ParamGridBuilder()\n",
    "                     .addGrid(rf.numTrees, input_numTrees.get(sel_stages))\n",
    "                     .addGrid(rf.maxDepth, input_maxDepth.get(sel_stages))                  \n",
    "                     .build())  \n",
    "\n",
    "        # Ejecuto la validación cruzada con los hiperparámetros seleccionados\n",
    "        cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, \n",
    "                            evaluator=evaluator, numFolds=5)\n",
    "        pipelineModel = cv.fit(df)    \n",
    "        \n",
    "        # Muestro los resultados\n",
    "        print('Hiper-parámetros óptimos:')\n",
    "        print('numTrees = ' + str(pipelineModel.bestModel.stages[-1]._java_obj.getNumTrees()))\n",
    "        print('maxDepth = ' + str(pipelineModel.bestModel.stages[-1]._java_obj.getMaxDepth()))\n",
    "        print('ROC-AUC = ' + str(np.mean(pipelineModel.avgMetrics)))\n",
    "        print('')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Coarse-tuning de hiper-parámetros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASO BASE\n",
      "------------------------\n",
      "Features: scaledFeatures\n",
      "Hiper-parámetros óptimos:\n",
      "numTrees = 200\n",
      "maxDepth = 15\n",
      "ROC-AUC = 0.7977902282992116\n",
      "\n",
      "CASO PCA-k5\n",
      "------------------------\n",
      "Features: pcaFeatures\n",
      "Hiper-parámetros óptimos:\n",
      "numTrees = 200\n",
      "maxDepth = 5\n",
      "ROC-AUC = 0.7088523134138865\n",
      "\n",
      "CASO PCA-k10\n",
      "------------------------\n",
      "Features: pcaFeatures\n",
      "Hiper-parámetros óptimos:\n",
      "numTrees = 300\n",
      "maxDepth = 10\n",
      "ROC-AUC = 0.7593208604087995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_numTrees = {'BASE': [50, 200, 300],\n",
    "                  'PCA-k5': [50, 200, 300],\n",
    "                  'PCA-k10': [50, 200, 300]}\n",
    "\n",
    "param_maxDepth = {'BASE': [5, 10, 15],\n",
    "                 'PCA-k5': [5, 10, 15],\n",
    "                 'PCA-k10': [5, 10, 15]}\n",
    "    \n",
    "evualua_modelo_RF(param_numTrees, param_maxDepth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Fine-tuning de hiper-parámetros y evaluación de la ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASO BASE\n",
      "------------------------\n",
      "Features: scaledFeatures\n",
      "Hiper-parámetros óptimos:\n",
      "numTrees = 220\n",
      "maxDepth = 15\n",
      "ROC-AUC = 0.8027401665016947\n",
      "\n",
      "CASO PCA-k5\n",
      "------------------------\n",
      "Features: pcaFeatures\n",
      "Hiper-parámetros óptimos:\n",
      "numTrees = 200\n",
      "maxDepth = 5\n",
      "ROC-AUC = 0.7176632423658518\n",
      "\n",
      "CASO PCA-k10\n",
      "------------------------\n",
      "Features: pcaFeatures\n",
      "Hiper-parámetros óptimos:\n",
      "numTrees = 320\n",
      "maxDepth = 8\n",
      "ROC-AUC = 0.7665432457275998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_numTrees = {'BASE': [180, 200, 220],\n",
    "                  'PCA-k5': [180, 200, 220],\n",
    "                  'PCA-k10': [280, 300, 320]}\n",
    "\n",
    "param_maxDepth = {'BASE': [13, 15, 17],\n",
    "                 'PCA-k5': [3, 5, 7],\n",
    "                 'PCA-k10': [8, 10, 12]}\n",
    "    \n",
    "evualua_modelo_RF(param_numTrees, param_maxDepth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5 Calculo la exactitud del mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8572428924529123\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "rf = RandomForestClassifier(featuresCol='scaledFeatures', labelCol='label')\n",
    "\n",
    "rf_stages = main_stages[:]\n",
    "rf_stages += [rf]\n",
    "pipeline = Pipeline(stages=rf_stages)\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.numTrees, [220])\n",
    "             .addGrid(rf.maxDepth, [15])     \n",
    "             .build())  \n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, \n",
    "                    evaluator=evaluator, numFolds=5)\n",
    "model = cv.fit(df)  \n",
    "print('Accuracy = ' + str(np.mean(model.avgMetrics)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6 Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El hiper-parámetro numTrees es el número de árboles que serán creados. Un valor alto puede provocar overfitting, mientras que uno bajo puede dar lungar al underfitting. El valor óptimo encontrado es 220 para el caso Base.\n",
    "\n",
    "El hiper-parámetro maxDepth representa la profundidad máxima de cada árbol. También en este caso, un valor muy alto causa overfitting. El valor óptimo encontrado es 15 para el caso Base.\n",
    "\n",
    "La mejor eficacia la obtuve con el caso sin selección ni extracción de variables (BASE). Esto tiene sentido, ya que los algoritmos basados en árboles de decisión tienen integrada la selección de las variables más predictivas.\n",
    "\n",
    "ROC-AUC = 0.8027\n",
    "\n",
    "Accuracy = 0.8572"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gradient Boosting Tree\n",
    "<a class=\"anchor\" id=\"gbt\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Creo diccionarios con la información necesaria para ejecutar todos los casos por cada aloritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stages = {'BASE': main_stages,\n",
    "              'PCA-k5': pca_stages_5,\n",
    "              'PCA-k10': pca_stages_10}\n",
    "\n",
    "feature_field = {'BASE': 'scaledFeatures',\n",
    "                 'PCA-k5': 'pcaFeatures',\n",
    "                 'PCA-k10': 'pcaFeatures'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: Gradient Boosted Tree es un algoritmo basado en árboles de decisión, los cuales realizan internamente la selección de variables. Por este motivo, excluyo los dos pipelines con selección de variables por chi-cuadrado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Creo una función que evalúa el algoritmo con los hiper-parámetros en entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evualua_modelo_GBT(input_maxIter, input_maxDepth):\n",
    "    evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "    for sel_stages in all_stages:\n",
    "        \n",
    "        print('CASO ' + sel_stages)\n",
    "        print('------------------------')\n",
    "        \n",
    "        # Creo el algoritmo de clasificación\n",
    "        gbt = GBTClassifier(featuresCol=feature_field.get(sel_stages), labelCol='label')\n",
    "        print('Features: ' + feature_field.get(sel_stages))\n",
    "        \n",
    "        # Construyo el pipeline completo\n",
    "        gbt_stages = all_stages.get(sel_stages)[:]\n",
    "        gbt_stages += [gbt]\n",
    "        pipeline = Pipeline(stages=gbt_stages)\n",
    "\n",
    "        # Creo el grid de hiper-parámetros\n",
    "        paramGrid = (ParamGridBuilder()\n",
    "                     .addGrid(gbt.maxIter, input_maxIter.get(sel_stages))\n",
    "                     .addGrid(gbt.maxDepth, input_maxDepth.get(sel_stages))                  \n",
    "                     .build())  \n",
    "\n",
    "        # Ejecuto la validación cruzada con los hiperparámetros seleccionados\n",
    "        cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, \n",
    "                            evaluator=evaluator, numFolds=5)\n",
    "        pipelineModel = cv.fit(df)    \n",
    "        \n",
    "        # Muestro los resultados\n",
    "        print('Hiper-parámetros óptimos:')\n",
    "        print('maxIter = ' + str(pipelineModel.bestModel.stages[-1]._java_obj.getMaxIter()))\n",
    "        print('maxDepth = ' + str(pipelineModel.bestModel.stages[-1]._java_obj.getMaxDepth()))\n",
    "        print('ROC-AUC = ' + str(np.mean(pipelineModel.avgMetrics)))\n",
    "        print('')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Coarse-tuning de hiper-parámetros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASO BASE\n",
      "------------------------\n",
      "Features: scaledFeatures\n",
      "Hiper-parámetros óptimos:\n",
      "maxIter = 80\n",
      "maxDepth = 2\n",
      "ROC-AUC = 0.7920814451235488\n",
      "\n",
      "CASO PCA-k5\n",
      "------------------------\n",
      "Features: pcaFeatures\n",
      "Hiper-parámetros óptimos:\n",
      "maxIter = 60\n",
      "maxDepth = 2\n",
      "ROC-AUC = 0.6965980448249756\n",
      "\n",
      "CASO PCA-k10\n",
      "------------------------\n",
      "Features: pcaFeatures\n",
      "Hiper-parámetros óptimos:\n",
      "maxIter = 80\n",
      "maxDepth = 2\n",
      "ROC-AUC = 0.7289748309589227\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_maxIter = {'BASE': [40, 60, 80],\n",
    "                 'PCA-k5': [40, 60, 80],\n",
    "                 'PCA-k10': [40, 60, 80]}\n",
    "\n",
    "param_maxDepth = {'BASE': [2, 4, 6],\n",
    "                 'PCA-k5': [2, 4, 6],\n",
    "                 'PCA-k10': [2, 4, 6],}\n",
    "    \n",
    "evualua_modelo_GBT(param_maxIter, param_maxDepth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4 Fine-tuning de hiper-parámetros y evaluación de la ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASO BASE\n",
      "------------------------\n",
      "Features: scaledFeatures\n",
      "Hiper-parámetros óptimos:\n",
      "maxIter = 140\n",
      "maxDepth = 2\n",
      "ROC-AUC = 0.8155666333735284\n",
      "\n",
      "CASO PCA-k5\n",
      "------------------------\n",
      "Features: pcaFeatures\n",
      "Hiper-parámetros óptimos:\n",
      "maxIter = 70\n",
      "maxDepth = 3\n",
      "ROC-AUC = 0.7111847107152408\n",
      "\n",
      "CASO PCA-k10\n",
      "------------------------\n",
      "Features: pcaFeatures\n",
      "Hiper-parámetros óptimos:\n",
      "maxIter = 90\n",
      "maxDepth = 2\n",
      "ROC-AUC = 0.7498059022616846\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_maxIter = {'BASE': [120, 130, 140],\n",
    "                  'PCA-k5': [60, 70, 80],\n",
    "                  'PCA-k10': [80, 90, 100]}\n",
    "\n",
    "param_maxDepth = {'BASE': [2, 3],\n",
    "                 'PCA-k5': [2, 3],\n",
    "                 'PCA-k10': [2, 3]}\n",
    "    \n",
    "evualua_modelo_GBT(param_maxIter, param_maxDepth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.5 Calculo la exactitud del mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8686679524939078\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "gbt = GBTClassifier(featuresCol='scaledFeatures', labelCol='label')\n",
    "\n",
    "gbt_stages = main_stages[:]\n",
    "gbt_stages += [gbt]\n",
    "pipeline = Pipeline(stages=gbt_stages)\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxIter, [140])\n",
    "             .addGrid(gbt.maxDepth, [2])     \n",
    "             .build())  \n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, \n",
    "                    evaluator=evaluator, numFolds=5)\n",
    "model = cv.fit(df)  \n",
    "print('Accuracy = ' + str(np.mean(model.avgMetrics)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5 Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El hiper-parámetro numIter es el número máximo de iteraciones. Un valor alto puede provocar overfitting, mientras que uno bajo puede dar lungar al underfitting. El valor óptimo encontrado es 220 para el caso Base.\n",
    "\n",
    "El hiper-parámetro maxDepth representa la profundidad máxima de cada árbol. También en este caso, un valor muy alto causa overfitting. El valor óptimo encontrado es 15 para el caso Base.\n",
    "\n",
    "Como con Random Forest, la mejor eficacia la obtuve sin selección ni extracción de variables, siendo basado en árboles de decisión.\n",
    "\n",
    "ROC-AUC = 0.8155\n",
    "\n",
    "Accuracy = 0.8686"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multilayer Perceptron\n",
    "<a class=\"anchor\" id=\"perceptron\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Creo diccionarios con la información necesaria para ejecutar todos los casos por cada aloritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stages = {'BASE': main_stages,\n",
    "              'CHI-CUADRADO-05': chisq_stages_05,\n",
    "              'CHI-CUADRADO-01': chisq_stages_01,\n",
    "              'PCA-k5': pca_stages_5,\n",
    "              'PCA-k10': pca_stages_10}\n",
    "\n",
    "feature_field = {'BASE': 'scaledFeatures',\n",
    "                 'CHI-CUADRADO-05': 'selectedFeatures',\n",
    "                 'CHI-CUADRADO-01': 'selectedFeatures',\n",
    "                 'PCA-k5': 'pcaFeatures',\n",
    "                 'PCA-k10': 'pcaFeatures'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Creo una función que evalúa el algoritmo con los hiper-parámetros en entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evualua_modelo_MP(input_maxIter, hidden_layer):\n",
    "    evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "    for sel_stages in all_stages:\n",
    "        \n",
    "        print('CASO ' + sel_stages)\n",
    "        print('------------------------')\n",
    "        \n",
    "        # Construyo el pipeline sin el clasificador para poder medir la cantidad de elementos\n",
    "        # en el array de entrada. Debe ser igual a la cantidad de neuronas de la primera capa.\n",
    "        mp_stages = all_stages.get(sel_stages)[:]\n",
    "        pipeline = Pipeline(stages=mp_stages)\n",
    "        pipelineModel = pipeline.fit(df)\n",
    "        df_features = pipelineModel.transform(df)\n",
    "        \n",
    "        # Construyo las 3 capas\n",
    "        num_features = len(df_features.select(feature_field.get(sel_stages)).take(1)[0][0])\n",
    "        layers = [num_features, hidden_layer.get(sel_stages), 2]\n",
    "\n",
    "        # Creo el algoritmo de clasificación\n",
    "        mp = MultilayerPerceptronClassifier(featuresCol=feature_field.get(sel_stages), \n",
    "                                            labelCol='label', layers=layers,\n",
    "                                            blockSize=8)\n",
    "        print('Features: ' + feature_field.get(sel_stages))\n",
    "        print('Layers: ' + str(layers))\n",
    "        \n",
    "        \n",
    "        # Creo el grid de hiper-parámetros\n",
    "        paramGrid = (ParamGridBuilder()\n",
    "                     .addGrid(mp.maxIter, input_maxIter.get(sel_stages))\n",
    "                     .build())  \n",
    "\n",
    "        # Ejecuto la validación cruzada con los hiperparámetros seleccionados\n",
    "        cv = CrossValidator(estimator=mp, estimatorParamMaps=paramGrid, \n",
    "                            evaluator=evaluator, numFolds=5)\n",
    "        model = cv.fit(df_features)    \n",
    "        \n",
    "        # Muestro los resultados\n",
    "        print('Hiper-parámetros óptimos:')\n",
    "        print('maxIter = ' + str(model.bestModel._java_obj.parent().getMaxIter()))\n",
    "        print('ROC-AUC = ' + str(np.mean(model.avgMetrics)))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 Coarse-tuning de hiper-parámetros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASO BASE\n",
      "------------------------\n",
      "Features: scaledFeatures\n",
      "Layers: [43, 22, 2]\n",
      "Hiper-parámetros óptimos:\n",
      "maxIter = 20\n",
      "ROC-AUC = 0.8075354100031692\n",
      "\n",
      "CASO CHI-CUADRADO-05\n",
      "------------------------\n",
      "Features: selectedFeatures\n",
      "Layers: [27, 13, 2]\n",
      "Hiper-parámetros óptimos:\n",
      "maxIter = 20\n",
      "ROC-AUC = 0.8012664327757114\n",
      "\n",
      "CASO CHI-CUADRADO-01\n",
      "------------------------\n",
      "Features: selectedFeatures\n",
      "Layers: [25, 12, 2]\n",
      "Hiper-parámetros óptimos:\n",
      "maxIter = 20\n",
      "ROC-AUC = 0.796781592603775\n",
      "\n",
      "CASO PCA-k5\n",
      "------------------------\n",
      "Features: pcaFeatures\n",
      "Layers: [5, 3, 2]\n",
      "Hiper-parámetros óptimos:\n",
      "maxIter = 20\n",
      "ROC-AUC = 0.6857973215519944\n",
      "\n",
      "CASO PCA-k10\n",
      "------------------------\n",
      "Features: pcaFeatures\n",
      "Layers: [10, 5, 2]\n",
      "Hiper-parámetros óptimos:\n",
      "maxIter = 20\n",
      "ROC-AUC = 0.7718899148835229\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_maxIter = {'BASE': [10, 20, 30],\n",
    "                 'CHI-CUADRADO-05': [10, 20, 30],\n",
    "                 'CHI-CUADRADO-01': [10, 20, 30],\n",
    "                 'PCA-k5': [10, 20, 30],\n",
    "                 'PCA-k10': [10, 20, 30]}\n",
    "\n",
    "param_layers = {'BASE': 22,\n",
    "                'CHI-CUADRADO-05': 13,\n",
    "                'CHI-CUADRADO-01': 12,\n",
    "                'PCA-k5': 3,\n",
    "                'PCA-k10': 5}\n",
    "    \n",
    "evualua_modelo_MP(param_maxIter, param_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4 Fine-tuning de hiper-parámetros y evaluación de la ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASO BASE\n",
      "------------------------\n",
      "Features: scaledFeatures\n",
      "Layers: [43, 22, 2]\n",
      "Hiper-parámetros óptimos:\n",
      "maxIter = 20\n",
      "ROC-AUC = 0.8210557007372139\n",
      "\n",
      "CASO CHI-CUADRADO-05\n",
      "------------------------\n",
      "Features: selectedFeatures\n",
      "Layers: [27, 13, 2]\n",
      "Hiper-parámetros óptimos:\n",
      "maxIter = 15\n",
      "ROC-AUC = 0.8058342862861996\n",
      "\n",
      "CASO CHI-CUADRADO-01\n",
      "------------------------\n",
      "Features: selectedFeatures\n",
      "Layers: [25, 12, 2]\n",
      "Hiper-parámetros óptimos:\n",
      "maxIter = 15\n",
      "ROC-AUC = 0.8040253316350424\n",
      "\n",
      "CASO PCA-k5\n",
      "------------------------\n",
      "Features: pcaFeatures\n",
      "Layers: [5, 3, 2]\n",
      "Hiper-parámetros óptimos:\n",
      "maxIter = 15\n",
      "ROC-AUC = 0.695810530500201\n",
      "\n",
      "CASO PCA-k10\n",
      "------------------------\n",
      "Features: pcaFeatures\n",
      "Layers: [10, 5, 2]\n",
      "Hiper-parámetros óptimos:\n",
      "maxIter = 15\n",
      "ROC-AUC = 0.774815000263342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_maxIter = {'BASE': [15, 20, 25],\n",
    "                 'CHI-CUADRADO-05': [15, 20, 25],\n",
    "                 'CHI-CUADRADO-01': [15, 20, 25],\n",
    "                 'PCA-k5': [15, 20, 25],\n",
    "                 'PCA-k10': [15, 20, 25]}\n",
    "\n",
    "param_layers = {'BASE': 22,\n",
    "                'CHI-CUADRADO-05': 13,\n",
    "                'CHI-CUADRADO-01': 12,\n",
    "                'PCA-k5': 3,\n",
    "                'PCA-k10': 5}\n",
    "    \n",
    "evualua_modelo_MP(param_maxIter, param_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5 Calculo la exactitud del mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.879859437233464\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "mp = MultilayerPerceptronClassifier(featuresCol='scaledFeatures', \n",
    "                                    labelCol='label', layers=[43, 22, 2], blockSize=8)\n",
    "\n",
    "mp_stages = main_stages[:]\n",
    "mp_stages += [mp]\n",
    "pipeline = Pipeline(stages=mp_stages)\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(mp.maxIter, [20])\n",
    "             .build())  \n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, \n",
    "                    evaluator=evaluator, numFolds=5)\n",
    "model = cv.fit(df)  \n",
    "print('Accuracy = ' + str(np.mean(model.avgMetrics)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.6 Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas las redes neuronales poseen 3 capas y la siguiente cantidad de neuronas por cada capa:\n",
    "* Primera: cantidad de features presente en el vector en entrada a la red neuronal. Depende de la ejecución de las técnicas de selección y extracción de variables.\n",
    "* Segunda o hidden layer: valor comprendido entre el número de neuronas de la primera y tercera capa. Realicé varias pruebas para encontrar el valor que diera mejor resultado.\n",
    "* Tercera: número de clases de la variable objetivo, o sea constante 2.\n",
    "\n",
    "El hiper-parámetro maxIter es el número máximo de iteraciones. Un valor alto puede provocar overfitting, mientras que uno bajo puede dar lungar al underfitting. El valor óptimo encontrado es 20 para el caso Base.\n",
    "\n",
    "Obtuve la mejor eficacia sin selección ni extracción de variables. No asombra este resultado, ya que las redes neuronales no son sensibles a la presencia variables correlacionadas.\n",
    "\n",
    "ROC-AUC = 0.8210\n",
    "\n",
    "Accuracy = 0.8798"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente tabla resume los resultados obtenidos con los 5 algorítmos de clasificación. Se encuentran ordenados según la eficacia obtenida con las dos métricas.\n",
    "\n",
    "| Algoritmo | Accuracy | ROC-AUC | Selección de variables | PCA |  \n",
    "| --- | --- | --- | --- | --- |\n",
    "| Multilayer Perceptron | 0.8798 | 0.8210 | No | No |\n",
    "| Gradient Boosting Trees | 0.8686 | 0.8155 | No | No |\n",
    "| Random Forest | 0.8572 | 0.8027 | No | No |\n",
    "| Support Vector Machines | 0.8385 | 0.8294 | No | No |\n",
    "| Bayes Ingénuo | 0.8435 | 0.6339 | Si | No |\n",
    "\n",
    "La red de neuronas ofrece la mayor exactitud y casi la mejor área bajo la curva ROC. \n",
    "Una exactitud de 88% es bastante alta para un modelo predictivo y el área bajo la curva de 0.82 nos garantiza que el modelo es eficaz para varios umbrales de TPR y FPR. \n",
    "\n",
    "Los algoritmos basados en ensemble, como Gradient Boosting y Random Forest, estuvieron muy cerca de la red de neuronas. Demuestra que son muy eficaces, ofreciendo además resultados más claros para el ser humano.\n",
    "\n",
    "Support Vector Machines, aún siendo un algoritmo relativamente básico, ha dado resultados excelentes para ambas métricas. En mi opinión, es el caso más sorprendente.\n",
    "\n",
    "Bayes Ingénuo, en cambio, tiene el área bajo la curva muy baja y la exactitud alta. Obviamente, he revisado el procedimiento varias veces y el resultado sigue siendo el mismo. Supongo que tiene ese nivel de exactitud en ciertas condiciones de TPR y FPR, mientras que en el resto es bastante inferior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
